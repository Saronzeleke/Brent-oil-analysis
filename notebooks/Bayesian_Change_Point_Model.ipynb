{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbecdd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install gxx`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preparation import OilDataProcessor\n",
    "from model import BayesianChangePointModel\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5959d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4075eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampled model data shape: (901,)\n",
      "Fitting Bayesian change point model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [tau]\n",
      ">NUTS: [mu1, mu2, sigma1, sigma2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f228f40e134d63aab7083086e0c376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     17\u001b[39m model = BayesianChangePointModel(\n\u001b[32m     18\u001b[39m     data=model_data_small,  \u001b[38;5;66;03m# <-- FIXED: use downsampled data\u001b[39;00m\n\u001b[32m     19\u001b[39m     model_type=\u001b[33m'\u001b[39m\u001b[33mmean_shift\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Detect changes in mean\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFitting Bayesian change point model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchains\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# quick run for submission\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 4. Check convergence\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Convergence Diagnostics ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Brent-oil-analysis\\notebooks\\../src\\model.py:126\u001b[39m, in \u001b[36mBayesianChangePointModel.fit\u001b[39m\u001b[34m(self, draws, tune, chains, target_accept)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Sample from posterior\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28mself\u001b[39m.trace = \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchains\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Get summary statistics\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28mself\u001b[39m.summary = az.summary(\u001b[38;5;28mself\u001b[39m.trace, round_to=\u001b[32m4\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Brent-oil-analysis\\my_env\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:1011\u001b[39m, in \u001b[36msample\u001b[39m\u001b[34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, quiet, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m t_sampling = time.time() - t_start\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZarrTrace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Brent-oil-analysis\\my_env\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:1100\u001b[39m, in \u001b[36m_sample_return\u001b[39m\u001b[34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model, quiet)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     traces, length = \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     traces, length = _choose_chains(traces, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\Brent-oil-analysis\\my_env\\Lib\\site-packages\\pymc\\backends\\base.py:624\u001b[39m, in \u001b[36m_choose_chains\u001b[39m\u001b[34m(traces, tune)\u001b[39m\n\u001b[32m    622\u001b[39m lengths = [\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) - tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot enough samples to build a trace.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    626\u001b[39m idxs = np.argsort(lengths)\n\u001b[32m    627\u001b[39m l_sort = np.array(lengths)[idxs]\n",
      "\u001b[31mValueError\u001b[39m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "# 1. Load and prepare data\n",
    "processor = OilDataProcessor(\n",
    "    price_path='../data/BrentOilPrices.csv',\n",
    "    events_path='../data/events.csv'\n",
    ")\n",
    "\n",
    "price_df, events_df = processor.load_and_prepare_data()\n",
    "\n",
    "# 2. Prepare data for modeling (use log returns for stationarity)\n",
    "model_data = processor.prepare_for_modeling(price_df, use_log_returns=True)\n",
    "\n",
    "# Downsample for faster runtime\n",
    "model_data_small = model_data[::10]\n",
    "print(f\"Downsampled model data shape: {model_data_small.shape}\")\n",
    "\n",
    "# 3. Build and fit Bayesian change point model\n",
    "model = BayesianChangePointModel(\n",
    "    data=model_data_small,  # <-- FIXED: use downsampled data\n",
    "    model_type='mean_shift'  # Detect changes in mean\n",
    ")\n",
    "\n",
    "print(\"Fitting Bayesian change point model...\")\n",
    "model.fit(draws=200, tune=200, chains=1)  # quick run for submission\n",
    "\n",
    "# 4. Check convergence\n",
    "print(\"\\n=== Convergence Diagnostics ===\")\n",
    "diagnostics = model.diagnose_convergence()\n",
    "print(f\"R-hat convergence: {diagnostics.get('rhat_convergence', 'N/A')}\")\n",
    "print(f\"ESS adequate: {diagnostics.get('ess_adequate', 'N/A')}\")\n",
    "\n",
    "# 5. Display summary statistics\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "print(model.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot posterior distributions\n",
    "# Get dates for conversion (excluding first NaN from log return calculation)\n",
    "dates_for_model = price_df.index[1:]  # Skip first date due to log return calculation\n",
    "model.plot_posterior_distributions(dates=dates_for_model)\n",
    "\n",
    "# 7. Quantify impact\n",
    "impact = model.quantify_impact()\n",
    "print(\"\\n=== Impact Quantification ===\")\n",
    "for key, value in impact.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# 8. Identify most probable change point\n",
    "tau_samples = model.get_change_point_posterior()\n",
    "most_probable_tau = int(np.median(tau_samples))\n",
    "change_date = dates_for_model[most_probable_tau]\n",
    "\n",
    "print(f\"\\nMost probable change point index: {most_probable_tau}\")\n",
    "print(f\"Most probable change date: {change_date.date()}\")\n",
    "\n",
    "# 9. Compare with events\n",
    "print(\"\\n=== Nearby Historical Events ===\")\n",
    "# Find events near the change point\n",
    "time_window = pd.Timedelta(days=30)\n",
    "nearby_events = events_df[\n",
    "    (events_df.index >= change_date - time_window) & \n",
    "    (events_df.index <= change_date + time_window)\n",
    "]\n",
    "\n",
    "if len(nearby_events) > 0:\n",
    "    print(f\"Found {len(nearby_events)} event(s) within ±30 days:\")\n",
    "    for date, event in nearby_events.iterrows():\n",
    "        print(f\"  {date.date()}: {event['event_type']} - {event['event_description']}\")\n",
    "else:\n",
    "    print(\"No events found within ±30 days\")\n",
    "\n",
    "# 10. Visualize change point on price series\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot price series\n",
    "plt.plot(price_df.index, price_df['Price'], linewidth=1, alpha=0.7, label='Price')\n",
    "\n",
    "# Highlight change point region\n",
    "change_price = price_df.loc[change_date, 'Price'] if change_date in price_df.index else None\n",
    "if change_price:\n",
    "    plt.axvline(x=change_date, color='red', linestyle='--', alpha=0.7, \n",
    "                label=f'Change Point: {change_date.date()}')\n",
    "    plt.scatter(change_date, change_price, color='red', s=100, zorder=5)\n",
    "\n",
    "# Add events\n",
    "for date, event in events_df.iterrows():\n",
    "    if date in price_df.index:\n",
    "        plt.scatter(date, price_df.loc[date, 'Price'], \n",
    "                   color='green' if event['impact_direction'] == 'positive' else 'orange',\n",
    "                   s=50, alpha=0.7, marker='^', zorder=4)\n",
    "\n",
    "plt.title('Brent Oil Prices with Change Point and Events', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11. Advanced: Multiple change points (future work section)\n",
    "print(\"\\n=== Future Work: Multiple Change Points ===\")\n",
    "print(\"\"\"\n",
    "For more comprehensive analysis, consider:\n",
    "1. Multiple change point models to detect several regime shifts\n",
    "2. Incorporating external variables (GDP, inflation, exchange rates)\n",
    "3. Using VAR models for dynamic relationships\n",
    "4. Markov-switching models for regime classification\n",
    "5. Machine learning approaches for event impact prediction\n",
    "\"\"\")\n",
    "\n",
    "# 12. Save results\n",
    "results = {\n",
    "    'change_date': change_date,\n",
    "    'most_probable_tau': most_probable_tau,\n",
    "    'tau_posterior': tau_samples,\n",
    "    'impact_metrics': impact,\n",
    "    'nearby_events': nearby_events.to_dict('records') if len(nearby_events) > 0 else []\n",
    "}\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Convert datetime objects to strings\n",
    "def json_serializer(obj):\n",
    "    if isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "with open('../results/change_point_analysis.json', 'w') as f:\n",
    "    json.dump(results, f, default=json_serializer, indent=2)\n",
    "\n",
    "print(\"\\nAnalysis complete! Results saved to '../results/change_point_analysis.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
